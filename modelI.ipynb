{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70dd5395-a670-4e06-abe8-8b1f1f1a3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a815d6d-78b0-438d-b864-1cf399cd0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device('mps')\n",
    "#     print(\"Apple GPU\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81e0799-36e5-4847-aef0-608acbc84a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data's shape is (10000, 50, 110, 6) and Test Data's is (2100, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "def getData(path):\n",
    "    train_file = np.load(path+\"/train.npz\")\n",
    "    train_data = train_file['data']\n",
    "    test_file = np.load(path+\"/test_input.npz\")\n",
    "    test_data = test_file['data']\n",
    "    print(f\"Training Data's shape is {train_data.shape} and Test Data's is {test_data.shape}\")\n",
    "    return train_data, test_data\n",
    "trainData, testData = getData(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a83249-e56c-4444-a1e8-0beb9510ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position X range: -9305.083 to 13261.225\n",
      "Position Y range: -4581.698 to 6655.741\n",
      "Velocity X range: -40.043 to 42.910\n",
      "Velocity Y range: -35.110 to 32.641\n"
     ]
    }
   ],
   "source": [
    "def compute_feature_ranges(data):\n",
    "    # Feature indices:\n",
    "    # 0: pos_x, 1: pos_y, 2: vel_x, 3: vel_y\n",
    "    pos_x = data[..., 0]\n",
    "    pos_y = data[..., 1]\n",
    "    vel_x = data[..., 2]\n",
    "    vel_y = data[..., 3]\n",
    "\n",
    "    print(f\"Position X range: {pos_x.min().item():.3f} to {pos_x.max().item():.3f}\")\n",
    "    print(f\"Position Y range: {pos_y.min().item():.3f} to {pos_y.max().item():.3f}\")\n",
    "    print(f\"Velocity X range: {vel_x.min().item():.3f} to {vel_x.max().item():.3f}\")\n",
    "    print(f\"Velocity Y range: {vel_y.min().item():.3f} to {vel_y.max().item():.3f}\")\n",
    "\n",
    "compute_feature_ranges(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55cbdd82-18a7-4d52-9353-a7a5cf40bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_x = trainData[..., 0]\n",
    "pos_y = trainData[..., 1]\n",
    "vel_x = trainData[..., 2]\n",
    "vel_y = trainData[..., 3]\n",
    "\n",
    "POS_X_MIN, POS_X_MAX = pos_x.min(), pos_x.max()\n",
    "POS_Y_MIN, POS_Y_MAX = pos_y.min(), pos_y.max()\n",
    "VEL_X_MIN, VEL_X_MAX = vel_x.min(), vel_x.max()\n",
    "VEL_Y_MIN, VEL_Y_MAX = vel_y.min(), vel_y.max()\n",
    "\n",
    "# print(f\"Position X range: {pos_x.min().item():.3f} to {pos_x.max().item():.3f}\")\n",
    "# print(f\"Position Y range: {pos_y.min().item():.3f} to {pos_y.max().item():.3f}\")\n",
    "# print(f\"Velocity X range: {vel_x.min().item():.3f} to {vel_x.max().item():.3f}\")\n",
    "# print(f\"Velocity Y range: {vel_y.min().item():.3f} to {vel_y.max().item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0b801a-adb8-445d-b7f0-73ab922392fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_batchX(data):\n",
    "    data = data.clone()\n",
    "    ref_t = 49\n",
    "\n",
    "    pos_x = data[..., 0]\n",
    "    pos_y = data[..., 1]\n",
    "    vel_x = data[..., 2]\n",
    "    vel_y = data[..., 3]\n",
    "    heading = data[..., 4]\n",
    "\n",
    "    x_ref = pos_x[:, :, ref_t].unsqueeze(-1)\n",
    "    y_ref = pos_y[:, :, ref_t].unsqueeze(-1)\n",
    "    theta_ref = heading[:, :, ref_t].unsqueeze(-1)\n",
    "\n",
    "    cos_theta = torch.cos(-theta_ref)\n",
    "    sin_theta = torch.sin(-theta_ref)\n",
    "\n",
    "    dx = pos_x - x_ref\n",
    "    dy = pos_y - y_ref\n",
    "\n",
    "    new_x = dx * cos_theta - dy * sin_theta\n",
    "    new_y = dx * sin_theta + dy * cos_theta\n",
    "    new_vx = vel_x * cos_theta - vel_y * sin_theta\n",
    "    new_vy = vel_x * sin_theta + vel_y * cos_theta\n",
    "    new_heading = (heading - theta_ref + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "\n",
    "    def minmax_norm(t, t_min, t_max):\n",
    "        return (t - t_min) / (t_max - t_min)\n",
    "\n",
    "    new_x = minmax_norm(new_x, POS_X_MIN, POS_X_MAX)\n",
    "    new_y = minmax_norm(new_y, POS_Y_MIN, POS_Y_MAX)\n",
    "    new_vx = minmax_norm(new_vx, VEL_X_MIN, VEL_X_MAX)\n",
    "    new_vy = minmax_norm(new_vy, VEL_Y_MIN, VEL_Y_MAX)\n",
    "\n",
    "    # Mask for timestamps where all features are zero (B, 50 agents, 50 timestamps)\n",
    "    all_zero_mask = torch.all(data == 0.0, dim=-1)\n",
    "    # all_zero_mask = torch.all(torch.isclose(batchY, torch.zeros_like(batchY)), dim=-1)\n",
    "    \n",
    "    # Apply transformation only where not all zeros\n",
    "    mask_inv = ~all_zero_mask\n",
    "\n",
    "    data[..., 0] = torch.where(mask_inv, new_x, data[..., 0])\n",
    "    data[..., 1] = torch.where(mask_inv, new_y, data[..., 1])\n",
    "    data[..., 2] = torch.where(mask_inv, new_vx, data[..., 2])\n",
    "    data[..., 3] = torch.where(mask_inv, new_vy, data[..., 3])\n",
    "    data[..., 4] = torch.where(mask_inv, new_heading, data[..., 4])\n",
    "\n",
    "    return data, x_ref, y_ref, theta_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc7c4b32-e363-4c78-88ea-5f594e2c2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_batchY(batchY, x_ref, y_ref, theta_ref):\n",
    "    batchY = batchY.clone()\n",
    "\n",
    "    cos_theta = torch.cos(-theta_ref).squeeze(-1)  # (B,50)\n",
    "    sin_theta = torch.sin(-theta_ref).squeeze(-1)  # (B,50)\n",
    "\n",
    "    cos_theta = cos_theta[:, 0].unsqueeze(-1)  # (B,1)\n",
    "    sin_theta = sin_theta[:, 0].unsqueeze(-1)  # (B,1)\n",
    "    x_ref = x_ref[:, 0, 0].unsqueeze(-1)       # (B,1)\n",
    "    y_ref = y_ref[:, 0, 0].unsqueeze(-1)       # (B,1)\n",
    "    theta_ref = theta_ref[:, 0, 0].unsqueeze(-1)  # (B,1)\n",
    "\n",
    "    dx = batchY[..., 0] - x_ref\n",
    "    dy = batchY[..., 1] - y_ref\n",
    "\n",
    "    new_x = dx * cos_theta - dy * sin_theta\n",
    "    new_y = dx * sin_theta + dy * cos_theta\n",
    "\n",
    "    new_vx = batchY[..., 2] * cos_theta - batchY[..., 3] * sin_theta\n",
    "    new_vy = batchY[..., 2] * sin_theta + batchY[..., 3] * cos_theta\n",
    "\n",
    "    new_heading = (batchY[..., 4] - theta_ref + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "\n",
    "    def minmax_norm(t, t_min, t_max):\n",
    "        return (t - t_min) / (t_max - t_min)\n",
    "\n",
    "    new_x = minmax_norm(new_x, POS_X_MIN, POS_X_MAX)\n",
    "    new_y = minmax_norm(new_y, POS_Y_MIN, POS_Y_MAX)\n",
    "    new_vx = minmax_norm(new_vx, VEL_X_MIN, VEL_X_MAX)\n",
    "    new_vy = minmax_norm(new_vy, VEL_Y_MIN, VEL_Y_MAX)\n",
    "\n",
    "    all_zero_mask = torch.all(batchY == 0.0, dim=-1)  # (B,60)\n",
    "    # all_zero_mask = torch.all(torch.isclose(batchY, torch.zeros_like(batchY)), dim=-1)\n",
    "\n",
    "\n",
    "    mask_inv = ~all_zero_mask\n",
    "\n",
    "    batchY[..., 0] = torch.where(mask_inv, new_x, batchY[..., 0])\n",
    "    batchY[..., 1] = torch.where(mask_inv, new_y, batchY[..., 1])\n",
    "    batchY[..., 2] = torch.where(mask_inv, new_vx, batchY[..., 2])\n",
    "    batchY[..., 3] = torch.where(mask_inv, new_vy, batchY[..., 3])\n",
    "    batchY[..., 4] = torch.where(mask_inv, new_heading, batchY[..., 4])\n",
    "\n",
    "    return batchY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4abd32a0-6cf6-4b10-9573-72496e8b72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_batchY(batchY_pred, x_ref, y_ref, theta_ref):\n",
    "    batchY_pred = batchY_pred.clone()\n",
    "\n",
    "    cos_theta = torch.cos(theta_ref).squeeze(-1)\n",
    "    sin_theta = torch.sin(theta_ref).squeeze(-1)\n",
    "\n",
    "    cos_theta = cos_theta[:, 0].unsqueeze(-1)\n",
    "    sin_theta = sin_theta[:, 0].unsqueeze(-1)\n",
    "    x_ref = x_ref[:, 0, 0].unsqueeze(-1)\n",
    "    y_ref = y_ref[:, 0, 0].unsqueeze(-1)\n",
    "    theta_ref = theta_ref[:, 0, 0].unsqueeze(-1)\n",
    "\n",
    "    x = batchY_pred[..., 0]\n",
    "    y = batchY_pred[..., 1]\n",
    "\n",
    "    def minmax_unnorm(t, t_min, t_max):\n",
    "        return t * (t_max - t_min) + t_min\n",
    "\n",
    "    xx = minmax_unnorm(x, POS_X_MIN, POS_X_MAX)\n",
    "    yy = minmax_unnorm(y, POS_Y_MIN, POS_Y_MAX)\n",
    "\n",
    "    vx = batchY_pred[..., 2]\n",
    "    vy = batchY_pred[..., 3]\n",
    "\n",
    "    vvx = minmax_unnorm(vx, VEL_X_MIN, VEL_X_MAX)\n",
    "    vvy = minmax_unnorm(vy, VEL_Y_MIN, VEL_Y_MAX)\n",
    "    \n",
    "\n",
    "    global_x = xx * cos_theta - yy * sin_theta + x_ref\n",
    "    global_y = xx * sin_theta + yy * cos_theta + y_ref\n",
    "\n",
    "    vx = batchY_pred[..., 2]\n",
    "    vy = batchY_pred[..., 3]\n",
    "\n",
    "    global_vx = vvx * cos_theta - vvy * sin_theta\n",
    "    global_vy = vvx * sin_theta + vvy * cos_theta\n",
    "\n",
    "    global_heading = (batchY_pred[..., 4] + theta_ref + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "\n",
    "    all_zero_mask = torch.all(batchY_pred == 0.0, dim=-1)\n",
    "    # all_zero_mask = torch.all(torch.isclose(batchY, torch.zeros_like(batchY)), dim=-1)\n",
    "\n",
    "\n",
    "    mask_inv = ~all_zero_mask\n",
    "\n",
    "    batchY_pred[..., 0] = torch.where(mask_inv, global_x, batchY_pred[..., 0])\n",
    "    batchY_pred[..., 1] = torch.where(mask_inv, global_y, batchY_pred[..., 1])\n",
    "    batchY_pred[..., 2] = torch.where(mask_inv, global_vx, batchY_pred[..., 2])\n",
    "    batchY_pred[..., 3] = torch.where(mask_inv, global_vy, batchY_pred[..., 3])\n",
    "    batchY_pred[..., 4] = torch.where(mask_inv, global_heading, batchY_pred[..., 4])\n",
    "\n",
    "    return batchY_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13b1669a-eb3a-4ba1-a196-59f97663c360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 14.8431,  85.2577, 109.9855,  95.4430, 149.1436, 152.8894])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempX = 100 * torch.rand(10, 50, 50, 6).to(device)\n",
    "tempY = 300 * torch.rand(10, 60, 6).to(device)\n",
    "\n",
    "tempY[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711509a0-2cd3-4fa2-9f94-1323ca7a459e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 14.8431,  85.2577, 109.9855,  95.4430, 149.1436, 152.8894]),\n",
       " tensor([  0.4130,   0.4153,   1.8086,   1.9270,  -1.6529, 152.8894]),\n",
       " tensor([ 14.8430,  85.2578, 109.9855,  95.4430,  -1.6529, 152.8894]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normX, x_ref, y_ref, theta_ref = normalize_batchX(tempX)\n",
    "normY = normalize_batchY(tempY, x_ref, y_ref, theta_ref)\n",
    "unNormY = unnormalize_batchY(normY, x_ref, y_ref, theta_ref)\n",
    "tempY[0, 0, :], normY[0, 0, :], unNormY[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4c9b62-6610-4f58-b52d-8011d2d8c86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 60, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (T, D)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()  # (T, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # (1, T, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "    def __init__(self, input_dim = 6, output_dim = 6, model_dim=128, num_heads=4, num_layers=3, dropout=0.1, pred_len=60):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.input_fc = nn.Linear(input_dim, model_dim)\n",
    "        self.temporal_encoding = PositionalEncoding(model_dim, max_len=128)\n",
    "\n",
    "        self.time_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Agent-wise encoder: (50 agents) * (50 time) → Agent embeddings\n",
    "        self.agent_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.query_embed = nn.Parameter(torch.randn(1, pred_len, model_dim))\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Final prediction: output 2D coordinates\n",
    "        self.output_fc = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 50 agents, 50 timesteps, f features)\n",
    "        B, N, T, F = x.shape\n",
    "\n",
    "        x = x.view(B * N, T, F)\n",
    "        x = self.input_fc(x)  \n",
    "        x = self.temporal_encoding(x) \n",
    "        # x = x.mean(dim=1)\n",
    "\n",
    "        # x = self.temporal_encoding(x)              # (B*N, T, D)\n",
    "        x = self.time_encoder(x)                   # (B*N, T, D)\n",
    "        x = x[:, -1, :]                            # Use last token representation (you could also try mean pooling)\n",
    "        # x = x.view(B, N, self.model_dim)\n",
    "\n",
    "        x = x.view(B, N, self.model_dim)\n",
    "        # print(x.shape)\n",
    "\n",
    "        encoded_agents = self.agent_encoder(x)  \n",
    "\n",
    "        agent0_embed = encoded_agents[:, :, :]  # (B, D)\n",
    "        # agent0_embed = agent0_embed.unsqueeze(1)  # (B, 1, D)\n",
    "\n",
    "        agent0_embed = encoded_agents[:, 0:1, :]  # shape: (B, 1, D)\n",
    "        query = agent0_embed.repeat(1, self.pred_len, 1)  # shape: (B, 60, D)\n",
    "        out = self.decoder(query, encoded_agents)\n",
    "        # query = self.query_embed.repeat(B, 1, 1)  # (B, 60, D)\n",
    "\n",
    "        # out = self.decoder(query, agent0_embed)  # (B, 60, D)\n",
    "\n",
    "        out = self.output_fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "model = TrajectoryTransformer()\n",
    "x = torch.randn(1, 50, 50, 6)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22af16eb-b0b9-4346-b557-2ef14318f6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 60, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (T, D)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()  # (T, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # (1, T, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=6, output_dim=6, model_dim=128, num_heads=4, num_layers=3, dropout=0.1, pred_len=60, num_agents=50):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.pred_len = pred_len\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "        self.input_fc = nn.Linear(input_dim, model_dim)\n",
    "        self.temporal_encoding = PositionalEncoding(model_dim, max_len=128)\n",
    "\n",
    "        self.time_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.agent_embedding = nn.Embedding(num_agents, model_dim)  # Option 2: agent ID embedding\n",
    "\n",
    "        self.agent_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.output_fc = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N=50 agents, T=50 timesteps, F=6 features)\n",
    "        B, N, T, F = x.shape\n",
    "\n",
    "        x = x.view(B * N, T, F)               # (B*N, T, F)\n",
    "        x = self.input_fc(x)                  # (B*N, T, D)\n",
    "        x = self.temporal_encoding(x)         # (B*N, T, D)\n",
    "        x = self.time_encoder(x)              # (B*N, T, D)\n",
    "        x = x[:, -1, :]                       # (B*N, D) → use last time step\n",
    "        x = x.view(B, N, self.model_dim)      # (B, N, D)\n",
    "\n",
    "        # Add agent identity embeddings (Option 2)\n",
    "        agent_ids = torch.arange(0, N, device=x.device).unsqueeze(0).repeat(B, 1)  # (B, N)\n",
    "        agent_id_embeds = self.agent_embedding(agent_ids)                         # (B, N, D)\n",
    "        x = x + agent_id_embeds                                                   # (B, N, D)\n",
    "\n",
    "        encoded_agents = self.agent_encoder(x)  # (B, N, D)\n",
    "\n",
    "        # Use 0th agent embedding as query (Option 1)\n",
    "        agent0_embed = encoded_agents[:, 0:1, :]                 # (B, 1, D)\n",
    "        query = agent0_embed.repeat(1, self.pred_len, 1)         # (B, 60, D)\n",
    "\n",
    "        out = self.decoder(query, encoded_agents)                # (B, 60, D)\n",
    "        out = self.output_fc(out)                                # (B, 60, output_dim)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Test run\n",
    "model = TrajectoryTransformer()\n",
    "x = torch.randn(1, 50, 50, 6)  # (B=1, 50 agents, 50 time, 6 features)\n",
    "out = model(x)\n",
    "print(out.shape)  # Expected: (1, 60, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61117038-0a12-44df-b8db-75c3c142c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5544198\n"
     ]
    }
   ],
   "source": [
    "model = TrajectoryTransformer().to(device=device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f9b4c04-efb7-4022-b2c3-728e9fdbeb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9000, 50, 110, 6)\n",
      "Validation shape: (1000, 50, 110, 6)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_samples = trainData.shape[0]\n",
    "indices = np.random.permutation(num_samples)\n",
    "split_index = int(0.9 * num_samples)\n",
    "train_idx, val_idx = indices[:split_index], indices[split_index:]\n",
    "\n",
    "# Split the data\n",
    "train_data = trainData[train_idx]\n",
    "val_data = trainData[val_idx]\n",
    "\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Validation shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99abe4e2-e3b0-4db0-bd9e-e41e95963318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9000, 50, 50, 6), (9000, 60, 6), (1000, 50, 50, 6), (1000, 60, 6))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX, trainY, validX, validY = train_data[:, :, :50, :], train_data[:, 0, 50:, :], val_data[:, :, :50, :], val_data[:, 0, 50:, :]\n",
    "trainX.shape, trainY.shape, validX.shape, validY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3af09b0-6980-4557-95c7-57f758eaaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_ = self.X[idx]\n",
    "        Y_ = self.Y[idx]\n",
    "        \n",
    "        return (torch.tensor(X_, dtype=torch.float32), \n",
    "                torch.tensor(Y_, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d8880f-ab51-46bd-81fa-b75484b82a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTensor = TensorDataset(trainX, trainY)\n",
    "testTensor = TensorDataset(validX, validY)\n",
    "\n",
    "train_dataloader = DataLoader(trainTensor, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(testTensor, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f016e95-5600-4a69-a121-42a26b7c1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = (input - target) ** 2\n",
    "        weighted_loss = loss * self.weights\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "feature_weights = torch.tensor([1.0, 1.0, 0.1, 0.1, 0.01, 0.01], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d0ee7d7-4379-4c3b-8e38-8f1bfaeeeace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000]: 100%|██████████| 282/282 [25:57<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0191030771 | train val MSE 0.0142462896 | val MAE 2104.3061027527 | val MSE 7279939.1562500000\n",
      " model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000]:  10%|▉         | 27/282 [02:31<23:51,  5.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m normalizedY = normalizedY.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# compute_feature_ranges(normalizedX)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# compute_feature_ranges(normalizedY)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalizedX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m loss = lossFn(pred[...,:], normalizedY[..., :]).to(device)\n\u001b[32m     38\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mTrajectoryTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     52\u001b[39m x = \u001b[38;5;28mself\u001b[39m.input_fc(x)                  \u001b[38;5;66;03m# (B*N, T, D)\u001b[39;00m\n\u001b[32m     53\u001b[39m x = \u001b[38;5;28mself\u001b[39m.temporal_encoding(x)         \u001b[38;5;66;03m# (B*N, T, D)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtime_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m              \u001b[38;5;66;03m# (B*N, T, D)\u001b[39;00m\n\u001b[32m     55\u001b[39m x = x[:, -\u001b[32m1\u001b[39m, :]                       \u001b[38;5;66;03m# (B*N, D) → use last time step\u001b[39;00m\n\u001b[32m     56\u001b[39m x = x.view(B, N, \u001b[38;5;28mself\u001b[39m.model_dim)      \u001b[38;5;66;03m# (B, N, D)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:517\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    514\u001b[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[32m    525\u001b[39m     output = output.to_padded_tensor(\u001b[32m0.0\u001b[39m, src.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:920\u001b[39m, in \u001b[36mTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    916\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m._ff_block(\u001b[38;5;28mself\u001b[39m.norm2(x))\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    918\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m    919\u001b[39m         x\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m         + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m     )\n\u001b[32m    922\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm2(x + \u001b[38;5;28mself\u001b[39m._ff_block(x))\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:943\u001b[39m, in \u001b[36mTransformerEncoderLayer._sa_block\u001b[39m\u001b[34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sa_block\u001b[39m(\n\u001b[32m    928\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    929\u001b[39m     x: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    932\u001b[39m     is_causal: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    933\u001b[39m ) -> Tensor:\n\u001b[32m    934\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    935\u001b[39m         x,\n\u001b[32m    936\u001b[39m         x,\n\u001b[32m   (...)\u001b[39m\u001b[32m    941\u001b[39m         is_causal=is_causal,\n\u001b[32m    942\u001b[39m     )[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "epochs = 1000\n",
    "# lossFn = nn.MSELoss()\n",
    "lossFn = WeightedMSELoss(feature_weights)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.25)\n",
    "best_val_loss = float('inf')\n",
    "best_train_loss = float('inf')\n",
    "position_scale = 1.0\n",
    "velocity_scale = 1.0\n",
    "\n",
    "all_losses = {\n",
    "    'training_mse_loss':[],\n",
    "    'validation_mse_loss':[],\n",
    "    'true_mse':[],\n",
    "    'true_mae':[]\n",
    "}\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    model.train()\n",
    "    runningLoss = 0.0\n",
    "    loop = tqdm(train_dataloader, desc=f\"Epoch [{each_epoch+1}/{epochs}]\")\n",
    "    for batchX, batchY in loop:\n",
    "        normalizedX, x_ref, y_ref, theta_ref = normalize_batchX(batchX) \n",
    "        normalizedY = normalize_batchY(batchY, x_ref, y_ref, theta_ref)\n",
    "        \n",
    "        normalizedX = normalizedX.to(device, non_blocking=True)   \n",
    "        normalizedY = normalizedY.to(device, non_blocking=True)  \n",
    "            \n",
    "        # compute_feature_ranges(normalizedX)\n",
    "        # compute_feature_ranges(normalizedY)\n",
    "        pred = model(normalizedX)\n",
    "        loss = lossFn(pred[...,:], normalizedY[..., :]).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        runningLoss += loss.item()        \n",
    "        # print(pred.shape)\n",
    "        # break\n",
    "    # break\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_mae = 0\n",
    "    val_mse = 0\n",
    "    with torch.no_grad():\n",
    "        for batchX, batchY in val_dataloader:   \n",
    "            normalizedX, x_ref, y_ref, theta_ref = normalize_batchX(batchX) \n",
    "            x_ref = x_ref.to(device)\n",
    "            y_ref = y_ref.to(device)\n",
    "            theta_ref = theta_ref.to(device)\n",
    "            batchX = batchX.to(device, non_blocking=True)   \n",
    "            batchY = batchY.to(device, non_blocking=True) \n",
    "            normalizedY = normalize_batchY(batchY, x_ref, y_ref, theta_ref)\n",
    "            \n",
    "            normalizedX = normalizedX.to(device, non_blocking=True)   \n",
    "            normalizedY = normalizedY.to(device, non_blocking=True)  \n",
    "\n",
    "            pred = model(normalizedX)\n",
    "            loss = lossFn(pred[...,:], normalizedY[..., :]).to(device)\n",
    "            \n",
    "            \n",
    "            unnorm_pred = unnormalize_batchY(pred, x_ref, y_ref, theta_ref)\n",
    "            unnorm_true = unnormalize_batchY(normalizedY, x_ref, y_ref, theta_ref)\n",
    "\n",
    "            # print(batchY[0, 34, :], '\\n', normalizedY[0, 34, :], '\\n', unnorm_pred[0, 34, :], '\\n',  unnorm_true[0, 34, :])\n",
    "            # break\n",
    "            val_loss += loss.item()\n",
    "            val_mae += nn.L1Loss()(unnorm_pred[..., :2], unnorm_true[..., :2]).item()\n",
    "            val_mse += nn.MSELoss()(unnorm_pred[..., :2], unnorm_true[..., :2]).item()\n",
    "    #         break\n",
    "    # break\n",
    "            \n",
    "    train_loss = runningLoss/len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_mae /= len(val_dataloader)\n",
    "    val_mse /= len(val_dataloader)\n",
    "    all_losses[\"training_mse_loss\"].append(train_loss)\n",
    "    all_losses[\"validation_mse_loss\"].append(val_loss)\n",
    "    all_losses[\"true_mse\"].append(val_mse)\n",
    "    all_losses[\"true_mae\"].append(val_mae)\n",
    "\n",
    "    loop.write(f\" train MSE {train_loss:.10f} | train val MSE {val_loss:.10f} | val MAE {val_mae:.10f} | val MSE {val_mse:.10f}\")\n",
    "    scheduler.step()\n",
    "    \n",
    "    if train_loss < best_train_loss and val_loss < best_val_loss - 1e-3:\n",
    "        best_val_loss = val_loss\n",
    "        best_train_loss = train_loss\n",
    "        no_improvement = 0\n",
    "        torch.save(model.state_dict(), \"./models/modelI/best_model.pt\")\n",
    "        loop.write(f\" model Saved\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # else:\n",
    "    #     no_improvement += 1\n",
    "    # break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca52e9a-5831-4156-9d29-c6bb7bd61b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9153dc-847a-4f85-8651-ed28fbdb8ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f41bc-3f92-47c3-8da8-291e2fdcb63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421d4be-f281-44b7-a0ea-87fb7770cb55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
