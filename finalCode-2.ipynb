{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5112b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e90c4fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Apple GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f367345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data's shape is (10000, 50, 110, 6) and Test Data's is (2100, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "def getData(path):\n",
    "    train_file = np.load(path+\"/train.npz\")\n",
    "    train_data = train_file['data']\n",
    "    test_file = np.load(path+\"/test_input.npz\")\n",
    "    test_data = test_file['data']\n",
    "    print(f\"Training Data's shape is {train_data.shape} and Test Data's is {test_data.shape}\")\n",
    "    return train_data, test_data\n",
    "trainData, testData = getData(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dac0b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedNormalizedDataset(Dataset):\n",
    "    def __init__(self, data, scale=10.0):\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "        self.dt = 0.1\n",
    "        self.interaction_radius = 15.0  # Meters for local interaction\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx].copy()\n",
    "        presence = (scene[..., 0] != 0) | (scene[..., 1] != 0)\n",
    "\n",
    "        origin = scene[0, 49].copy()\n",
    "        tx, ty, _, _, theta, _ = origin\n",
    "\n",
    "        cos_theta = np.cos(-theta)\n",
    "        sin_theta = np.sin(-theta)\n",
    "\n",
    "        # Allocate space for all features (0-13 = 14 features total)\n",
    "        normalized_scene = np.zeros((50, 110, 14), dtype=np.float32)\n",
    "\n",
    "        # --- Existing normalization (features 0-10) ---\n",
    "        # Position normalization\n",
    "        x = scene[..., 0] - tx\n",
    "        y = scene[..., 1] - ty\n",
    "        x_n = x * cos_theta - y * sin_theta\n",
    "        y_n = x * sin_theta + y * cos_theta\n",
    "        normalized_scene[..., 0] = x_n / self.scale\n",
    "        normalized_scene[..., 1] = y_n / self.scale\n",
    "\n",
    "        # Velocity normalization\n",
    "        vx = scene[..., 2]\n",
    "        vy = scene[..., 3]\n",
    "        vx_n = vx * cos_theta - vy * sin_theta\n",
    "        vy_n = vx * sin_theta + vy * cos_theta\n",
    "        normalized_scene[..., 2] = vx_n / self.scale\n",
    "        normalized_scene[..., 3] = vy_n / self.scale\n",
    "\n",
    "        # Heading normalization\n",
    "        heading = scene[..., 4]\n",
    "        normalized_heading = heading - theta\n",
    "        normalized_heading = (normalized_heading + np.pi) % (2 * np.pi) - np.pi\n",
    "        normalized_scene[..., 4] = normalized_heading\n",
    "\n",
    "        # Agent type and presence\n",
    "        normalized_scene[..., 5] = scene[..., 5]\n",
    "        normalized_scene[..., 6] = presence.astype(np.float32)\n",
    "\n",
    "        # Speed\n",
    "        speed = np.sqrt(vx ** 2 + vy ** 2)\n",
    "        normalized_scene[..., 7] = speed / self.scale\n",
    "\n",
    "        # Distance to ego\n",
    "        ego_pos = scene[0, :, :2]\n",
    "        dist_to_ego = np.linalg.norm(scene[..., :2] - ego_pos[None, :, :], axis=-1)\n",
    "        normalized_scene[..., 8] = dist_to_ego / self.scale\n",
    "\n",
    "        # Acceleration\n",
    "        accel = np.zeros_like(speed)\n",
    "        accel[:, 1:] = (speed[:, 1:] - speed[:, :-1]) / self.dt\n",
    "        accel[:, 0] = accel[:, 1]\n",
    "        normalized_scene[..., 9] = accel / (self.scale / self.dt)\n",
    "\n",
    "        # TTC\n",
    "        rel_speed = np.sqrt((vx - vx[0:1])**2 + (vy - vy[0:1])**2)\n",
    "        ttc = dist_to_ego / (rel_speed + 1e-5)\n",
    "        ttc = np.clip(ttc, 0, 10)\n",
    "        normalized_scene[..., 10] = ttc / 10.0\n",
    "\n",
    "        # === NEW FEATURE 1: Dynamic Interaction Density (Optimized) ===\n",
    "        positions = scene[..., :2]\n",
    "        interaction_density = np.zeros((50, 110), dtype=np.float32)\n",
    "        \n",
    "        # Vectorized implementation\n",
    "        for t in range(110):\n",
    "            # Compute pairwise distances at timestep t\n",
    "            dist_matrix = np.linalg.norm(positions[:, t, None] - positions[:, t], axis=-1)\n",
    "            # Count neighbors within radius (excluding self)\n",
    "            neighbor_counts = np.sum((dist_matrix < self.interaction_radius) & (dist_matrix > 0), axis=-1)\n",
    "            interaction_density[:, t] = neighbor_counts\n",
    "            \n",
    "        normalized_scene[..., 11] = interaction_density / 20.0  # Normalize\n",
    "\n",
    "        # === NEW FEATURE 2: Future Motion Context ===\n",
    "        # Vector from current position to last observed position\n",
    "        future_context = np.zeros((50, 110, 2), dtype=np.float32)\n",
    "        \n",
    "        for agent in range(50):\n",
    "            valid_timesteps = np.where(presence[agent, :50])[0]\n",
    "            if len(valid_timesteps) > 0:\n",
    "                last_idx = valid_timesteps[-1]\n",
    "                goal_vector = positions[agent, last_idx] - positions[agent]\n",
    "                # Rotate to ego frame\n",
    "                goal_x = goal_vector[..., 0] * cos_theta - goal_vector[..., 1] * sin_theta\n",
    "                goal_y = goal_vector[..., 0] * sin_theta + goal_vector[..., 1] * cos_theta\n",
    "                future_context[agent, :, 0] = goal_x / self.scale\n",
    "                future_context[agent, :, 1] = goal_y / self.scale\n",
    "                \n",
    "        normalized_scene[..., 12] = future_context[..., 0]  # Relative goal X\n",
    "        normalized_scene[..., 13] = future_context[..., 1]  # Relative goal Y\n",
    "\n",
    "        # --- Masking ---\n",
    "        missing_mask = np.expand_dims(~presence, -1)\n",
    "        normalized_scene = np.where(missing_mask, 0, normalized_scene)\n",
    "\n",
    "        # Inputs: first 50 timesteps\n",
    "        X = normalized_scene[:, :50, :]  # Shape: (50 agents, 50 timesteps, 14 features)\n",
    "\n",
    "        # Target: ego future positions and presence\n",
    "        ego_future = normalized_scene[0, 50:]\n",
    "        Y = np.zeros((60, 3), dtype=np.float32)\n",
    "        Y[:, :2] = ego_future[:, :2]\n",
    "        Y[:, 2] = ego_future[:, 6]  # presence\n",
    "\n",
    "        return (\n",
    "            torch.tensor(X, dtype=torch.float32),\n",
    "            torch.tensor(Y, dtype=torch.float32),\n",
    "            torch.tensor(origin, dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "274659b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedNormalizedTestDataset(Dataset):\n",
    "    def __init__(self, data, scale=10.0):\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "        self.dt = 0.1\n",
    "        self.interaction_radius = 15.0  # Meters for local interaction\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx].copy()\n",
    "        presence = (scene[..., 0] != 0) | (scene[..., 1] != 0)\n",
    "\n",
    "        origin = scene[0, 49].copy()\n",
    "        tx, ty, _, _, theta, _ = origin\n",
    "\n",
    "        cos_theta = np.cos(-theta)\n",
    "        sin_theta = np.sin(-theta)\n",
    "\n",
    "        # Allocate space for all features (0-13 = 14 features total)\n",
    "        normalized_scene = np.zeros((50, 50, 14), dtype=np.float32)\n",
    "\n",
    "        # --- Existing normalization (features 0-10) ---\n",
    "        # Position normalization\n",
    "        x = scene[..., 0] - tx\n",
    "        y = scene[..., 1] - ty\n",
    "        x_n = x * cos_theta - y * sin_theta\n",
    "        y_n = x * sin_theta + y * cos_theta\n",
    "        normalized_scene[..., 0] = x_n / self.scale\n",
    "        normalized_scene[..., 1] = y_n / self.scale\n",
    "\n",
    "        # Velocity normalization\n",
    "        vx = scene[..., 2]\n",
    "        vy = scene[..., 3]\n",
    "        vx_n = vx * cos_theta - vy * sin_theta\n",
    "        vy_n = vx * sin_theta + vy * cos_theta\n",
    "        normalized_scene[..., 2] = vx_n / self.scale\n",
    "        normalized_scene[..., 3] = vy_n / self.scale\n",
    "\n",
    "        # Heading normalization\n",
    "        heading = scene[..., 4]\n",
    "        normalized_heading = heading - theta\n",
    "        normalized_heading = (normalized_heading + np.pi) % (2 * np.pi) - np.pi\n",
    "        normalized_scene[..., 4] = normalized_heading\n",
    "\n",
    "        # Agent type and presence\n",
    "        normalized_scene[..., 5] = scene[..., 5]\n",
    "        normalized_scene[..., 6] = presence.astype(np.float32)\n",
    "\n",
    "        # Speed\n",
    "        speed = np.sqrt(vx ** 2 + vy ** 2)\n",
    "        normalized_scene[..., 7] = speed / self.scale\n",
    "\n",
    "        # Distance to ego\n",
    "        ego_pos = scene[0, :, :2]\n",
    "        dist_to_ego = np.linalg.norm(scene[..., :2] - ego_pos[None, :, :], axis=-1)\n",
    "        normalized_scene[..., 8] = dist_to_ego / self.scale\n",
    "\n",
    "        # Acceleration\n",
    "        accel = np.zeros_like(speed)\n",
    "        accel[:, 1:] = (speed[:, 1:] - speed[:, :-1]) / self.dt\n",
    "        accel[:, 0] = accel[:, 1]\n",
    "        normalized_scene[..., 9] = accel / (self.scale / self.dt)\n",
    "\n",
    "        # TTC\n",
    "        rel_speed = np.sqrt((vx - vx[0:1])**2 + (vy - vy[0:1])**2)\n",
    "        ttc = dist_to_ego / (rel_speed + 1e-5)\n",
    "        ttc = np.clip(ttc, 0, 10)\n",
    "        normalized_scene[..., 10] = ttc / 10.0\n",
    "\n",
    "        # === NEW FEATURE 1: Dynamic Interaction Density (Optimized) ===\n",
    "        positions = scene[..., :2]\n",
    "        interaction_density = np.zeros((50, 50), dtype=np.float32)\n",
    "        \n",
    "        # Vectorized implementation\n",
    "        for t in range(50):\n",
    "            # Compute pairwise distances at timestep t\n",
    "            dist_matrix = np.linalg.norm(positions[:, t, None] - positions[:, t], axis=-1)\n",
    "            # Count neighbors within radius (excluding self)\n",
    "            neighbor_counts = np.sum((dist_matrix < self.interaction_radius) & (dist_matrix > 0), axis=-1)\n",
    "            interaction_density[:, t] = neighbor_counts\n",
    "            \n",
    "        normalized_scene[..., 11] = interaction_density / 20.0  # Normalize\n",
    "\n",
    "        # === NEW FEATURE 2: Future Motion Context ===\n",
    "        # Vector from current position to last observed position\n",
    "        future_context = np.zeros((50, 50, 2), dtype=np.float32)\n",
    "        \n",
    "        for agent in range(50):\n",
    "            valid_timesteps = np.where(presence[agent, :50])[0]\n",
    "            if len(valid_timesteps) > 0:\n",
    "                last_idx = valid_timesteps[-1]\n",
    "                goal_vector = positions[agent, last_idx] - positions[agent]\n",
    "                # Rotate to ego frame\n",
    "                goal_x = goal_vector[..., 0] * cos_theta - goal_vector[..., 1] * sin_theta\n",
    "                goal_y = goal_vector[..., 0] * sin_theta + goal_vector[..., 1] * cos_theta\n",
    "                future_context[agent, :, 0] = goal_x / self.scale\n",
    "                future_context[agent, :, 1] = goal_y / self.scale\n",
    "                \n",
    "        normalized_scene[..., 12] = future_context[..., 0]  # Relative goal X\n",
    "        normalized_scene[..., 13] = future_context[..., 1]  # Relative goal Y\n",
    "\n",
    "        # --- Masking ---\n",
    "        missing_mask = np.expand_dims(~presence, -1)\n",
    "        normalized_scene = np.where(missing_mask, 0, normalized_scene)\n",
    "\n",
    "        # Inputs: first 50 timesteps\n",
    "        X = normalized_scene[:, :50, :]  # Shape: (50 agents, 50 timesteps, 14 features)\n",
    "\n",
    "        # Target: ego future positions and presence\n",
    "        ego_future = normalized_scene[0, 50:]\n",
    "        Y = np.zeros((60, 3), dtype=np.float32)\n",
    "        Y[:, :2] = ego_future[:, :2]\n",
    "        Y[:, 2] = ego_future[:, 6]  # presence\n",
    "\n",
    "        return (\n",
    "            torch.tensor(X, dtype=torch.float32),\n",
    "            torch.tensor(Y, dtype=torch.float32),\n",
    "            torch.tensor(origin, dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec660861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_ego_batch(predicted, origin, scale=10.0):\n",
    "    \"\"\"\n",
    "    Convert batch of normalized (and scaled) ego predictions back to global coordinates.\n",
    "\n",
    "    predicted: (B, ..., 2) tensor of normalized [x, y] positions\n",
    "    origin: (B, 6) tensor of ego's reference state at t=49\n",
    "    Returns:\n",
    "        (B, ..., 2) tensor of global [x, y] positions\n",
    "    \"\"\"\n",
    "    tx = origin[:, 0]  # (B,)\n",
    "    ty = origin[:, 1]  # (B,)\n",
    "    theta = origin[:, 4]  # (B,)\n",
    "\n",
    "    cos_theta = torch.cos(theta)\n",
    "    sin_theta = torch.sin(theta)\n",
    "\n",
    "    # Expand for broadcasting\n",
    "    while len(cos_theta.shape) < len(predicted.shape) - 1:\n",
    "        cos_theta = cos_theta.unsqueeze(1)\n",
    "        sin_theta = sin_theta.unsqueeze(1)\n",
    "        tx = tx.unsqueeze(1)\n",
    "        ty = ty.unsqueeze(1)\n",
    "\n",
    "    # Unscale before denormalizing\n",
    "    x = predicted[..., 0] * scale\n",
    "    y = predicted[..., 1] * scale\n",
    "\n",
    "    # Rotate\n",
    "    x_rot = x * cos_theta - y * sin_theta\n",
    "    y_rot = x * sin_theta + y * cos_theta\n",
    "\n",
    "    # Translate\n",
    "    x_global = x_rot + tx\n",
    "    y_global = y_rot + ty\n",
    "\n",
    "    return torch.stack([x_global, y_global], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fca8c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.16906469e+03,  1.68248551e+03,  5.46145515e+00, -5.85380650e+00,\n",
       "        -8.22467566e-01,  0.00000000e+00]),\n",
       " array([ 3.16959927e+03,  1.68191109e+03,  5.35655550e+00, -5.75120145e+00,\n",
       "        -8.22600550e-01,  0.00000000e+00]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[1, 0, 49, :], trainData[1, 0, 50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "491f32c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000,  0.0000,  0.8006,  0.0019,  0.0000,  0.0000,  1.0000,  0.8006,\n",
       "          0.0000, -0.0057,  0.0000,  0.1000,  0.0000,  0.0000]),\n",
       " tensor([7.8468e-02, 9.1270e-05, 1.0000e+00]),\n",
       " torch.Size([6]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = WindowedNormalizedDataset(trainData)\n",
    "X, Y, origin = data.__getitem__(1)\n",
    "X[0, 49, :], Y[0, :], origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3ab6cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = denormalize_ego(Y[0, :2], origin)\n",
    "# x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a42736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 50, 50, 14])\n",
      "Output shape: torch.Size([1, 60, 2])\n",
      "\n",
      "Model parameters: 8,299,640\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=700, model_dim=256, num_heads=8, num_layers=6, dropout=0.1, pred_len=60, num_agents=50):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.pred_len = pred_len\n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        # Process each agent's full trajectory (50*7 = 350) into a single token\n",
    "        self.trajectory_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, model_dim),\n",
    "            nn.LayerNorm(model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.LayerNorm(model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.LayerNorm(model_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 2-layer transformer encoder to process agent tokens\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=model_dim, \n",
    "                nhead=num_heads, \n",
    "                dropout=dropout, \n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.output_fcpre = nn.Linear(model_dim, model_dim)  # 60*2 = 120\n",
    "        self.output_fc = nn.Linear(model_dim, pred_len * 2)  # 60*2 = 120\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, T, Ft = x.shape\n",
    "        x = x.view(B, N, T * Ft)  # (B, 50, 350)\n",
    "        agent_tokens = self.trajectory_encoder(x)  # (B, 50, model_dim)\n",
    "        encoded_tokens = self.transformer_encoder(agent_tokens)  # (B, 50, model_dim)\n",
    "        ego_token = encoded_tokens[:, 0, :]  # (B, model_dim)\n",
    "        output = F.relu(self.output_fcpre(ego_token))  # (B, pred_len*2)\n",
    "        output = self.output_fc(output)  # (B, pred_len*2)\n",
    "        output = output.view(B, self.pred_len, 2)  # (B, 60, 2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test run\n",
    "model = TrajectoryTransformer()\n",
    "x = torch.randn(1, 50, 50, 14)  \n",
    "out = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")  # Expected: (1, 60, 2)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3b77d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8299640\n"
     ]
    }
   ],
   "source": [
    "model = TrajectoryTransformer().to(device=device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18bd0a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9000, 50, 110, 6)\n",
      "Validation shape: (1000, 50, 110, 6)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_samples = trainData.shape[0]\n",
    "indices = np.random.permutation(num_samples)\n",
    "split_index = int(0.9 * num_samples)\n",
    "train_idx, val_idx = indices[:split_index], indices[split_index:]\n",
    "\n",
    "# Split the data\n",
    "train_data = trainData[train_idx]\n",
    "val_data = trainData[val_idx]\n",
    "\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Validation shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e6170bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTensor = WindowedNormalizedDataset(train_data)\n",
    "testTensor = WindowedNormalizedDataset(val_data)\n",
    "train_dataloader = DataLoader(trainTensor, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(testTensor, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09b473be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000]: 100%|██████████| 71/71 [01:03<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0492038001 | train val MSE 0.2322185440 | val MAE 8.0665035844 | val MSE 23.2218529880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000]: 100%|██████████| 71/71 [01:02<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0367420487 | train val MSE 0.2909632560 | val MAE 8.6918457448 | val MSE 29.0963252932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000]: 100%|██████████| 71/71 [01:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0341824244 | train val MSE 0.2625099593 | val MAE 8.7058800459 | val MSE 26.2509958744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000]: 100%|██████████| 71/71 [01:02<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0450145945 | train val MSE 0.2544256200 | val MAE 7.9182016477 | val MSE 25.4425592124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000]: 100%|██████████| 71/71 [01:02<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0404877220 | train val MSE 0.2096429870 | val MAE 7.0626409352 | val MSE 20.9642980695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/1000]: 100%|██████████| 71/71 [01:02<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0354927041 | train val MSE 0.2427074809 | val MAE 8.1183192879 | val MSE 24.2707504034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000]: 100%|██████████| 71/71 [01:02<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0298250480 | train val MSE 0.1819045418 | val MAE 7.1723596454 | val MSE 18.1904557496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/1000]: 100%|██████████| 71/71 [01:00<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0356939194 | train val MSE 0.2269439222 | val MAE 7.8091204837 | val MSE 22.6943914741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/1000]: 100%|██████████| 71/71 [01:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0293306330 | train val MSE 0.2225598118 | val MAE 7.7248474956 | val MSE 22.2559795827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000]: 100%|██████████| 71/71 [01:00<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0322138825 | train val MSE 0.2196888078 | val MAE 7.7514707297 | val MSE 21.9688845724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/1000]: 100%|██████████| 71/71 [01:02<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0322388119 | train val MSE 0.2506072728 | val MAE 8.6975185946 | val MSE 25.0607281923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/1000]: 100%|██████████| 71/71 [01:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0291709699 | train val MSE 0.2392361572 | val MAE 7.7979869321 | val MSE 23.9236205518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/1000]: 100%|██████████| 71/71 [00:59<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train MSE 0.0288806218 | train val MSE 0.2738061054 | val MAE 8.5432129279 | val MSE 27.3806151152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/1000]:  24%|██▍       | 17/71 [00:15<00:49,  1.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m runningLoss = \u001b[32m0.0\u001b[39m\n\u001b[32m     26\u001b[39m loop = tqdm(train_dataloader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meach_epoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatchX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatchX\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatchY\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mWindowedNormalizedDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Vectorized implementation\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m110\u001b[39m):\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# Compute pairwise distances at timestep t\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     dist_matrix = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# Count neighbors within radius (excluding self)\u001b[39;00m\n\u001b[32m     81\u001b[39m     neighbor_counts = np.sum((dist_matrix < \u001b[38;5;28mself\u001b[39m.interaction_radius) & (dist_matrix > \u001b[32m0\u001b[39m), axis=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#  train MSE 0.0148209710 | train val MSE 0.0955192002 | val MAE 4.9818070605 | val MSE 9.5519190058\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "best_model = torch.load(\"./models/modelI/best_model.pt\")\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "epochs = 1000\n",
    "lossFn = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.25)\n",
    "best_val_loss = 0.0955192002 # float('inf')\n",
    "best_train_loss = 0.0148209710 # float('inf')\n",
    "position_scale = 1.0\n",
    "velocity_scale = 1.0\n",
    "all_losses = {\n",
    "    'training_mse_loss':[],\n",
    "    'validation_mse_loss':[],\n",
    "    'true_mse':[],\n",
    "    'true_mae':[]\n",
    "}\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    model.train()\n",
    "    runningLoss = 0.0\n",
    "    loop = tqdm(train_dataloader, desc=f\"Epoch [{each_epoch+1}/{epochs}]\")\n",
    "    \n",
    "    for batchX, batchY, origin in loop:\n",
    "        batchX = batchX.to(device)\n",
    "        batchY = batchY.to(device)\n",
    "        origin = origin.to(device)\n",
    "\n",
    "        \n",
    "        pred = model(batchX)  # pred shape: (B, 60, 2)\n",
    "        \n",
    "        loss = lossFn(pred[..., :2], batchY[..., :2]).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        runningLoss += loss.item()        \n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_mae = 0\n",
    "    val_mse = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batchX, batchY, origin in loop:\n",
    "            batchX = batchX.to(device)\n",
    "            batchY = batchY.to(device)\n",
    "            origin = origin.to(device)\n",
    "\n",
    "            \n",
    "            pred = model(batchX)  # pred shape: (B, 60, 2)\n",
    "            \n",
    "            loss = lossFn(pred[..., :2], batchY[..., :2]).to(device)\n",
    "            unnorm_pred = denormalize_ego_batch(pred, origin)\n",
    "            unnorm_true = denormalize_ego_batch(batchY, origin)\n",
    "\n",
    "            # print(pred[..., :2].shape, batchY[..., :2].shape, origin.shape, unnorm_pred.shape)\n",
    "            \n",
    "            # break\n",
    "            \n",
    "            # unnorm_pred = denormalize_ego(pred[..., :2], origin)\n",
    "            # unnorm_true = denormalize_ego(batchY, origin)\n",
    "\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_mae += nn.L1Loss()(unnorm_pred[..., :2], unnorm_true[..., :2]).item()\n",
    "            val_mse += nn.MSELoss()(unnorm_pred[..., :2], unnorm_true[..., :2]).item()\n",
    "    # break\n",
    "    train_loss = runningLoss/len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_mae /= len(val_dataloader)\n",
    "    val_mse /= len(val_dataloader)\n",
    "    \n",
    "    all_losses[\"training_mse_loss\"].append(train_loss)\n",
    "    all_losses[\"validation_mse_loss\"].append(val_loss)\n",
    "    all_losses[\"true_mse\"].append(val_mse)\n",
    "    all_losses[\"true_mae\"].append(val_mae)\n",
    "    \n",
    "    loop.write(f\" train MSE {train_loss:.10f} | train val MSE {val_loss:.10f} | val MAE {val_mae:.10f} | val MSE {val_mse:.10f}\")\n",
    "    scheduler.step()\n",
    "    \n",
    "    if train_loss < best_train_loss and val_loss < best_val_loss: # - 1e-3\n",
    "        best_val_loss = val_loss\n",
    "        best_train_loss = train_loss\n",
    "        no_improvement = 0\n",
    "        torch.save(model.state_dict(), \"./models/modelI/best_model.pt\")\n",
    "        loop.write(f\" model Saved\")\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b40f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WindowedNormalizedTestDataset(testData)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "best_model = torch.load(\"./models/modelI/best_model.pt\")\n",
    "model = model = TrajectoryTransformer().to(device=device)\n",
    "\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "pred_list = []\n",
    "with torch.no_grad():\n",
    "    for batchX, origin in test_loader:\n",
    "        batchX = batchX.to(device)\n",
    "        batchY = batchY.to(device)\n",
    "        origin = origin.to(device)\n",
    "\n",
    "        \n",
    "        pred = model(batchX)  # pred shape: (B, 60, 2)\n",
    "        \n",
    "        unnorm_pred = denormalize_ego_batch(pred[..., :2], origin)\n",
    "        # print(unnorm_pred.shape)\n",
    "        pred_list.append(unnorm_pred.cpu().numpy())\n",
    "        # print(len(pred))\n",
    "        \n",
    "\n",
    "pred_list = np.concatenate(pred_list, axis=0)  \n",
    "pred_output = pred_list.reshape(-1, 2)  # (N*60, 2)\n",
    "output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
    "output_df.index.name = 'index'\n",
    "output_df.to_csv('./models/final/testTransFormer.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
