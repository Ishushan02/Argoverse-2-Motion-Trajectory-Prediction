{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b554cc-2500-48ac-98af-73f959d09a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b328dfab-1847-4f3f-bda2-4c917232d056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Apple GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf78a07-ee75-414a-a2df-a3cf74e5c800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data's shape is (10000, 50, 110, 6) and Test Data's is (2100, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "def getData(path):\n",
    "    train_file = np.load(path+\"/train.npz\")\n",
    "    train_data = train_file['data']\n",
    "    test_file = np.load(path+\"/test_input.npz\")\n",
    "    test_data = test_file['data']\n",
    "    print(f\"Training Data's shape is {train_data.shape} and Test Data's is {test_data.shape}\")\n",
    "    return train_data, test_data\n",
    "trainData, testData = getData(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f72e2f9-f680-48ab-9a24-a83571e63358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedNormalizedDataset(Dataset):\n",
    "    def __init__(self, data, scale=10.0):\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "        self.dt = 0.1  # Assuming 0.1s timesteps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx].copy()  # (50 agents, 110 timesteps, 6 features)\n",
    "        presence = (scene[..., 0] != 0) | (scene[..., 1] != 0)  # (50, 110)\n",
    "\n",
    "        origin = scene[0, 49].copy()\n",
    "        tx, ty, _, _, theta, _ = origin\n",
    "\n",
    "        cos_theta = np.cos(-theta)\n",
    "        sin_theta = np.sin(-theta)\n",
    "\n",
    "        # Create feature tensor with 14 features\n",
    "        normalized_scene = np.zeros((50, 110, 14), dtype=np.float32)\n",
    "\n",
    "        # --- Existing normalization (features 0-8) ---\n",
    "        # Positions\n",
    "        x = scene[..., 0] - tx\n",
    "        y = scene[..., 1] - ty\n",
    "        x_n = x * cos_theta - y * sin_theta\n",
    "        y_n = x * sin_theta + y * cos_theta\n",
    "        normalized_scene[..., 0] = x_n / self.scale\n",
    "        normalized_scene[..., 1] = y_n / self.scale\n",
    "        \n",
    "        # Velocities\n",
    "        vx = scene[..., 2]\n",
    "        vy = scene[..., 3]\n",
    "        vx_n = vx * cos_theta - vy * sin_theta\n",
    "        vy_n = vx * sin_theta + vy * cos_theta\n",
    "        normalized_scene[..., 2] = vx_n / self.scale\n",
    "        normalized_scene[..., 3] = vy_n / self.scale\n",
    "        \n",
    "        # Heading\n",
    "        heading = scene[..., 4]\n",
    "        normalized_heading = heading - theta\n",
    "        normalized_heading = (normalized_heading + np.pi) % (2 * np.pi) - np.pi\n",
    "        normalized_scene[..., 4] = normalized_heading\n",
    "        \n",
    "        # Agent type and presence\n",
    "        normalized_scene[..., 5] = scene[..., 5]  # agent_type\n",
    "        normalized_scene[..., 6] = presence.astype(np.float32)  # presence\n",
    "        \n",
    "        # Speed\n",
    "        speed = np.sqrt(vx ** 2 + vy ** 2)\n",
    "        normalized_scene[..., 7] = speed / self.scale\n",
    "        \n",
    "        # Distance to ego\n",
    "        ego_pos = scene[0, :, :2]  # (110, 2)\n",
    "        dist_to_ego = np.linalg.norm(scene[..., :2] - ego_pos[None, :, :], axis=-1)\n",
    "        normalized_scene[..., 8] = dist_to_ego / self.scale\n",
    "\n",
    "        # === New Feature 1: Minimum Distance to Any Agent ===\n",
    "        min_dists = np.full((50, 110), 1000.0)  # Initialize with large distance\n",
    "        positions = scene[..., :2]  # Original positions (50, 110, 2)\n",
    "        \n",
    "        for t in range(110):\n",
    "            # Only consider present agents\n",
    "            present_agents = np.where(presence[:, t])[0]\n",
    "            if len(present_agents) < 2:\n",
    "                continue  # Skip if less than 2 agents present\n",
    "                \n",
    "            # Get present agents' positions\n",
    "            pos_t = positions[present_agents, t, :]\n",
    "            \n",
    "            # Compute pairwise distances\n",
    "            diff = pos_t[:, None, :] - pos_t[None, :, :]\n",
    "            dist_matrix = np.linalg.norm(diff, axis=-1)\n",
    "            \n",
    "            # Ignore self-distance\n",
    "            np.fill_diagonal(dist_matrix, np.inf)\n",
    "            \n",
    "            # Find minimum distances\n",
    "            agent_min_dists = np.min(dist_matrix, axis=1)\n",
    "            \n",
    "            # Update only present agents\n",
    "            min_dists[present_agents, t] = agent_min_dists\n",
    "        \n",
    "        normalized_scene[..., 9] = min_dists / self.scale  # Feature index 9\n",
    "\n",
    "        # === New Feature 2: Acceleration Magnitude ===\n",
    "        acceleration = np.zeros_like(speed)\n",
    "        # Forward difference for acceleration\n",
    "        acceleration[:, 1:] = (speed[:, 1:] - speed[:, :-1]) / self.dt\n",
    "        # Handle first timestep\n",
    "        acceleration[:, 0] = acceleration[:, 1]  \n",
    "        normalized_scene[..., 10] = acceleration / self.scale  # Feature index 10\n",
    "\n",
    "        # === New Feature 3: Time-to-Collision (TTC) with Ego ===\n",
    "        # Relative velocity magnitude\n",
    "        rel_vel = np.sqrt((vx - vx[0])**2 + (vy - vy[0])**2)\n",
    "        # Avoid division by zero\n",
    "        ttc = np.full((50, 110), 10.0)  # Default to max TTC (10s)\n",
    "        valid_mask = (dist_to_ego > 0.1) & (rel_vel > 0.1)\n",
    "        ttc[valid_mask] = dist_to_ego[valid_mask] / rel_vel[valid_mask]\n",
    "        # Clip to meaningful range (0-10s)\n",
    "        ttc = np.clip(ttc, 0, 10)  \n",
    "        normalized_scene[..., 11] = ttc / 10.0  # Feature index 11 (scaled 0-1)\n",
    "\n",
    "        # === Optional: Ego Agent Flag ===\n",
    "        ego_mask = (np.arange(50) == 0).astype(np.float32)[:, None]\n",
    "        normalized_scene[..., 12] = ego_mask  # Feature index 12\n",
    "\n",
    "        # === Optional: Future Velocity Angle ===\n",
    "        future_vel_angle = np.zeros((50, 110))\n",
    "        future_vel_angle[:, :-1] = np.arctan2(vy_n[:, 1:], vx_n[:, 1:])\n",
    "        normalized_scene[..., 13] = future_vel_angle  # Feature index 13\n",
    "\n",
    "        # Mask out invalid timesteps\n",
    "        missing_mask = np.expand_dims(~presence, -1)\n",
    "        normalized_scene = np.where(missing_mask, 0, normalized_scene)\n",
    "\n",
    "        # Inputs: first 50 timesteps\n",
    "        X = normalized_scene[:, :50, :]  # (50 agents, 50 timesteps, 14 features)\n",
    "\n",
    "        # Target: ego agent's future positions and presence\n",
    "        ego_future = normalized_scene[0, 50:]\n",
    "        Y = np.zeros((60, 3), dtype=np.float32)\n",
    "        Y[:, :2] = ego_future[:, :2]  # Normalized positions\n",
    "        Y[:, 2] = ego_future[:, 6]    # Presence\n",
    "\n",
    "        return (\n",
    "            torch.tensor(X, dtype=torch.float32),\n",
    "            torch.tensor(Y, dtype=torch.float32),\n",
    "            torch.tensor(origin, dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9eadf117-228c-4f2e-a595-0dbff4a096f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedNormalizedTestDataset(Dataset):\n",
    "    def __init__(self, data, scale=10.0):\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "        self.dt = 0.1  # Assuming 0.1s timesteps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx].copy()  # (50 agents, 110 timesteps, 6 features)\n",
    "        presence = (scene[..., 0] != 0) | (scene[..., 1] != 0)  # (50, 110)\n",
    "\n",
    "        origin = scene[0, 49].copy()\n",
    "        tx, ty, _, _, theta, _ = origin\n",
    "\n",
    "        cos_theta = np.cos(-theta)\n",
    "        sin_theta = np.sin(-theta)\n",
    "\n",
    "        # Create feature tensor with 14 features\n",
    "        normalized_scene = np.zeros((50, 50, 14), dtype=np.float32)\n",
    "\n",
    "        # --- Existing normalization (features 0-8) ---\n",
    "        # Positions\n",
    "        x = scene[..., 0] - tx\n",
    "        y = scene[..., 1] - ty\n",
    "        x_n = x * cos_theta - y * sin_theta\n",
    "        y_n = x * sin_theta + y * cos_theta\n",
    "        normalized_scene[..., 0] = x_n / self.scale\n",
    "        normalized_scene[..., 1] = y_n / self.scale\n",
    "        \n",
    "        # Velocities\n",
    "        vx = scene[..., 2]\n",
    "        vy = scene[..., 3]\n",
    "        vx_n = vx * cos_theta - vy * sin_theta\n",
    "        vy_n = vx * sin_theta + vy * cos_theta\n",
    "        normalized_scene[..., 2] = vx_n / self.scale\n",
    "        normalized_scene[..., 3] = vy_n / self.scale\n",
    "        \n",
    "        # Heading\n",
    "        heading = scene[..., 4]\n",
    "        normalized_heading = heading - theta\n",
    "        normalized_heading = (normalized_heading + np.pi) % (2 * np.pi) - np.pi\n",
    "        normalized_scene[..., 4] = normalized_heading\n",
    "        \n",
    "        # Agent type and presence\n",
    "        normalized_scene[..., 5] = scene[..., 5]  # agent_type\n",
    "        normalized_scene[..., 6] = presence.astype(np.float32)  # presence\n",
    "        \n",
    "        # Speed\n",
    "        speed = np.sqrt(vx ** 2 + vy ** 2)\n",
    "        normalized_scene[..., 7] = speed / self.scale\n",
    "        \n",
    "        # Distance to ego\n",
    "        ego_pos = scene[0, :, :2]  # (110, 2)\n",
    "        dist_to_ego = np.linalg.norm(scene[..., :2] - ego_pos[None, :, :], axis=-1)\n",
    "        normalized_scene[..., 8] = dist_to_ego / self.scale\n",
    "\n",
    "        # === New Feature 1: Minimum Distance to Any Agent ===\n",
    "        min_dists = np.full((50, 50), 1000.0)  # Initialize with large distance\n",
    "        positions = scene[..., :2]  # Original positions (50, 110, 2)\n",
    "        \n",
    "        for t in range(50):\n",
    "            # Only consider present agents\n",
    "            present_agents = np.where(presence[:, t])[0]\n",
    "            if len(present_agents) < 2:\n",
    "                continue  # Skip if less than 2 agents present\n",
    "                \n",
    "            # Get present agents' positions\n",
    "            pos_t = positions[present_agents, t, :]\n",
    "            \n",
    "            # Compute pairwise distances\n",
    "            diff = pos_t[:, None, :] - pos_t[None, :, :]\n",
    "            dist_matrix = np.linalg.norm(diff, axis=-1)\n",
    "            \n",
    "            # Ignore self-distance\n",
    "            np.fill_diagonal(dist_matrix, np.inf)\n",
    "            \n",
    "            # Find minimum distances\n",
    "            agent_min_dists = np.min(dist_matrix, axis=1)\n",
    "            \n",
    "            # Update only present agents\n",
    "            min_dists[present_agents, t] = agent_min_dists\n",
    "        \n",
    "        normalized_scene[..., 9] = min_dists / self.scale  # Feature index 9\n",
    "\n",
    "        # === New Feature 2: Acceleration Magnitude ===\n",
    "        acceleration = np.zeros_like(speed)\n",
    "        # Forward difference for acceleration\n",
    "        acceleration[:, 1:] = (speed[:, 1:] - speed[:, :-1]) / self.dt\n",
    "        # Handle first timestep\n",
    "        acceleration[:, 0] = acceleration[:, 1]  \n",
    "        normalized_scene[..., 10] = acceleration / self.scale  # Feature index 10\n",
    "\n",
    "        # === New Feature 3: Time-to-Collision (TTC) with Ego ===\n",
    "        # Relative velocity magnitude\n",
    "        rel_vel = np.sqrt((vx - vx[0])**2 + (vy - vy[0])**2)\n",
    "        # Avoid division by zero\n",
    "        ttc = np.full((50, 50), 10.0)  # Default to max TTC (10s)\n",
    "        valid_mask = (dist_to_ego > 0.1) & (rel_vel > 0.1)\n",
    "        ttc[valid_mask] = dist_to_ego[valid_mask] / rel_vel[valid_mask]\n",
    "        # Clip to meaningful range (0-10s)\n",
    "        ttc = np.clip(ttc, 0, 10)  \n",
    "        normalized_scene[..., 11] = ttc / 10.0  # Feature index 11 (scaled 0-1)\n",
    "\n",
    "        # === Optional: Ego Agent Flag ===\n",
    "        ego_mask = (np.arange(50) == 0).astype(np.float32)[:, None]\n",
    "        normalized_scene[..., 12] = ego_mask  # Feature index 12\n",
    "\n",
    "        # === Optional: Future Velocity Angle ===\n",
    "        future_vel_angle = np.zeros((50, 50))\n",
    "        future_vel_angle[:, :-1] = np.arctan2(vy_n[:, 1:], vx_n[:, 1:])\n",
    "        normalized_scene[..., 13] = future_vel_angle  # Feature index 13\n",
    "\n",
    "        # Mask out invalid timesteps\n",
    "        missing_mask = np.expand_dims(~presence, -1)\n",
    "        normalized_scene = np.where(missing_mask, 0, normalized_scene)\n",
    "\n",
    "        # Inputs: first 50 timesteps\n",
    "        X = normalized_scene[:, :50, :]  # (50 agents, 50 timesteps, 14 features)\n",
    "\n",
    "        # Target: ego agent's future positions and presence\n",
    "        # ego_future = normalized_scene[0, 50:]\n",
    "        # Y = np.zeros((60, 3), dtype=np.float32)\n",
    "        # Y[:, :2] = ego_future[:, :2]  # Normalized positions\n",
    "        # Y[:, 2] = ego_future[:, 6]    # Presence\n",
    "\n",
    "        return (\n",
    "            torch.tensor(X, dtype=torch.float32),\n",
    "            # torch.tensor(Y, dtype=torch.float32),\n",
    "            torch.tensor(origin, dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c26c470e-3645-4bcc-8c35-09774c8c8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_ego_batch(predicted, origin, scale=10.0):\n",
    "    \"\"\"\n",
    "    Convert batch of normalized (and scaled) ego predictions back to global coordinates.\n",
    "\n",
    "    predicted: (B, ..., 2) tensor of normalized [x, y] positions\n",
    "    origin: (B, 6) tensor of ego's reference state at t=49\n",
    "    Returns:\n",
    "        (B, ..., 2) tensor of global [x, y] positions\n",
    "    \"\"\"\n",
    "    tx = origin[:, 0]  # (B,)\n",
    "    ty = origin[:, 1]  # (B,)\n",
    "    theta = origin[:, 4]  # (B,)\n",
    "\n",
    "    cos_theta = torch.cos(theta)\n",
    "    sin_theta = torch.sin(theta)\n",
    "\n",
    "    # Expand for broadcasting\n",
    "    while len(cos_theta.shape) < len(predicted.shape) - 1:\n",
    "        cos_theta = cos_theta.unsqueeze(1)\n",
    "        sin_theta = sin_theta.unsqueeze(1)\n",
    "        tx = tx.unsqueeze(1)\n",
    "        ty = ty.unsqueeze(1)\n",
    "\n",
    "    # Unscale before denormalizing\n",
    "    x = predicted[..., 0] * scale\n",
    "    y = predicted[..., 1] * scale\n",
    "\n",
    "    # Rotate\n",
    "    x_rot = x * cos_theta - y * sin_theta\n",
    "    y_rot = x * sin_theta + y * cos_theta\n",
    "\n",
    "    # Translate\n",
    "    x_global = x_rot + tx\n",
    "    y_global = y_rot + ty\n",
    "\n",
    "    return torch.stack([x_global, y_global], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b698a36-6836-4114-a4ac-745265b21283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000,  0.0000,  0.8006,  0.0019,  0.0000,  0.0000,  1.0000,  0.8006,\n",
       "          0.0000,  0.8294, -0.0571,  1.0000,  1.0000,  0.0016]),\n",
       " tensor([7.8468e-02, 9.1270e-05, 1.0000e+00]),\n",
       " torch.Size([6]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = WindowedNormalizedDataset(trainData)\n",
    "X, Y, origin = data.__getitem__(1)\n",
    "X[0, 49, :], Y[0, :], origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "054067a5-39a6-4d7b-a72c-733c8b27bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 50, 50, 14])\n",
      "Output shape: torch.Size([1, 60, 2])\n",
      "\n",
      "Model parameters: 8,299,640\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=700, model_dim=256, num_heads=8, num_layers=6, dropout=0.1, pred_len=60, num_agents=50):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.pred_len = pred_len\n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        # Process each agent's full trajectory (50*7 = 350) into a single token\n",
    "        self.trajectory_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, model_dim),\n",
    "            nn.LayerNorm(model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.LayerNorm(model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.LayerNorm(model_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 2-layer transformer encoder to process agent tokens\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=model_dim, \n",
    "                nhead=num_heads, \n",
    "                dropout=dropout, \n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Final linear layer to predict ego vehicle trajectory\n",
    "        self.output_fcpre = nn.Linear(model_dim, model_dim)  # 60*2 = 120\n",
    "        self.output_fc = nn.Linear(model_dim, pred_len * 2)  # 60*2 = 120\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, T, Ft = x.shape\n",
    "        \n",
    "        x = x.view(B, N, T * Ft)  # (B, 50, 350)\n",
    "        \n",
    "        # Encode each agent's trajectory into a token\n",
    "        agent_tokens = self.trajectory_encoder(x)  # (B, 50, model_dim)\n",
    "        \n",
    "        # Process all agent tokens through transformer\n",
    "        encoded_tokens = self.transformer_encoder(agent_tokens)  # (B, 50, model_dim)\n",
    "        \n",
    "        # Extract ego vehicle token (assuming agent 0 is ego)\n",
    "        ego_token = encoded_tokens[:, 0, :]  # (B, model_dim)\n",
    "        \n",
    "        # Predict ego trajectory\n",
    "        output = F.relu(self.output_fcpre(ego_token))  # (B, pred_len*2)\n",
    "\n",
    "        output = self.output_fc(output)  # (B, pred_len*2)\n",
    "        \n",
    "        # Reshape to (B, pred_len, 2)\n",
    "        output = output.view(B, self.pred_len, 2)  # (B, 60, 2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test run\n",
    "model = TrajectoryTransformer()\n",
    "x = torch.randn(1, 50, 50, 14)  \n",
    "out = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")  # Expected: (1, 60, 2)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "daeacf4c-5c20-4d07-bec7-59b3882a3c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8299640\n"
     ]
    }
   ],
   "source": [
    "model = TrajectoryTransformer().to(device=device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97d72603-95a3-4c9e-a684-1710481b50ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9000, 50, 110, 6)\n",
      "Validation shape: (1000, 50, 110, 6)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_samples = trainData.shape[0]\n",
    "indices = np.random.permutation(num_samples)\n",
    "split_index = int(0.9 * num_samples)\n",
    "train_idx, val_idx = indices[:split_index], indices[split_index:]\n",
    "\n",
    "# Split the data\n",
    "train_data = trainData[train_idx]\n",
    "val_data = trainData[val_idx]\n",
    "\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Validation shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4da16df3-a288-4ccd-942f-ebe299a4f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTensor = WindowedNormalizedDataset(train_data)\n",
    "testTensor = WindowedNormalizedDataset(val_data)\n",
    "train_dataloader = DataLoader(trainTensor, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(testTensor, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ceaaea0-c7d7-498a-84fd-f3f58e99bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WindowedNormalizedTestDataset(testData)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "best_model = torch.load(\"./models/final/best_model.pt\")\n",
    "model = model = TrajectoryTransformer().to(device=device)\n",
    "# model = model = TrajectoryTransformerPlus().to(device=device)\n",
    "\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "pred_list = []\n",
    "with torch.no_grad():\n",
    "    for batchX, origin in test_loader:\n",
    "        batchX = batchX.to(device)\n",
    "        # batchY = batchY.to(device)\n",
    "        origin = origin.to(device)\n",
    "\n",
    "        \n",
    "        pred = model(batchX)  # pred shape: (B, 60, 2)\n",
    "        \n",
    "        unnorm_pred = denormalize_ego_batch(pred[..., :2], origin)\n",
    "        # print(unnorm_pred.shape)\n",
    "        pred_list.append(unnorm_pred.cpu().numpy())\n",
    "        # print(len(pred))\n",
    "        \n",
    "\n",
    "pred_list = np.concatenate(pred_list, axis=0)  \n",
    "pred_output = pred_list.reshape(-1, 2)  # (N*60, 2)\n",
    "output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
    "output_df.index.name = 'index'\n",
    "output_df.to_csv('./models/final/TransFormer.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817b43e-4deb-4182-b52b-7471af4b28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  train MSE 0.0044924512 | train val MSE 0.0257930420 | val MAE 2.8316981792 | val MSE 2.5793063380 - test 8.29930\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
